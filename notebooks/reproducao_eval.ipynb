{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "from data.load_data import LoadData\n",
    "from models.baseline import ResnetBaseline\n",
    "\n",
    "from models.moe import ResnetMoE\n",
    "from utils import synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_label = 'gate'\n",
    "model_label = 'moe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_label == 'gate':\n",
    "    from configs.gate import LoadDataConfig, Downstream_cnn_args\n",
    "\n",
    "    loader_config = LoadDataConfig()\n",
    "    resnet_config = Downstream_cnn_args()\n",
    "\n",
    "    dataloader = LoadData(**loader_config.__dict__)\n",
    "    reference = ResnetBaseline(**resnet_config.__dict__)\n",
    "\n",
    "if model_label == 'moe':\n",
    "    from configs.baseline import LoadDataConfig\n",
    "    from configs.moe import MoE_cnn_args\n",
    "\n",
    "    loader_config = LoadDataConfig()\n",
    "    moe_config = MoE_cnn_args()\n",
    "\n",
    "    dataloader = LoadData(**loader_config.__dict__)\n",
    "    reference = ResnetMoE(**moe_config.__dict__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshould"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('output/conjugado_{}.pt'.format(model_label))\n",
    "# assert model.state_dict().keys() == reference.state_dict().keys()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/272 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [05:00<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5619128949615713,\n",
       "  0.8166281755196305,\n",
       "  0.7946127946127947,\n",
       "  0.6692975532754538,\n",
       "  0.720658888126287,\n",
       "  0.7494577006507592],\n",
       " [0.29, 0.37, 0.71, 0.34, 0.3, 0.32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dl = dataloader.get_val_dataloader()\n",
    "best_f1s, best_thresholds = synthesis(model, val_dl, None, device)\n",
    "best_f1s, best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# from utils import get_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 6\n",
    "# thresholds = np.arange(0, 1.01, 0.01)  # Array of thresholds from 0 to 1 with step 0.01\n",
    "# predictions = {thresh: [[] for _ in range(num_classes)] for thresh in thresholds}\n",
    "# true_labels_dict = [[] for _ in range(num_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dl = dataloader.get_val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for val_batch in tqdm(val_dl):\n",
    "#         raw, exam_id, label = val_batch\n",
    "#         ecg = get_inputs(raw).to(device)\n",
    "#         label = label.to(device).float()\n",
    "\n",
    "#         logits = model(ecg)\n",
    "#         probs = torch.sigmoid(logits)\n",
    "\n",
    "#         for class_idx in range(num_classes):\n",
    "#             for thresh in thresholds:\n",
    "#                 predicted_binary = (probs[:, class_idx] >= thresh).float()\n",
    "#                 predictions[thresh][class_idx].extend(\n",
    "#                     predicted_binary.cpu().numpy()\n",
    "#                 )\n",
    "#             true_labels_dict[class_idx].extend(\n",
    "#                 label[:, class_idx].cpu().numpy()\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_thresholds = [0.5] * num_classes\n",
    "# best_f1s = [0.0] * num_classes\n",
    "\n",
    "# for class_idx in (range(num_classes)):\n",
    "#     for thresh in thresholds:\n",
    "#         f1 = f1_score(\n",
    "#             true_labels_dict[class_idx],\n",
    "#             predictions[thresh][class_idx],\n",
    "#             zero_division=0,\n",
    "#         )\n",
    "\n",
    "#         if f1 > best_f1s[class_idx]:\n",
    "#             best_f1s[class_idx] = f1\n",
    "#             best_thresholds[class_idx] = thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_f1s, best_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [03:35<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': [0.9826927529520723,\n",
       "  0.9877865246584857,\n",
       "  0.9923593424403797,\n",
       "  0.988075943505441,\n",
       "  0.9882495948136143,\n",
       "  0.9854132901134521],\n",
       " 'Precision': [0.5245398773006135,\n",
       "  0.7418300653594772,\n",
       "  0.7703180212014135,\n",
       "  0.5730994152046783,\n",
       "  0.6895787139689579,\n",
       "  0.6524822695035462],\n",
       " 'Recall': [0.5428571428571428,\n",
       "  0.8954635108481263,\n",
       "  0.7649122807017544,\n",
       "  0.765625,\n",
       "  0.8315508021390374,\n",
       "  0.8679245283018868],\n",
       " 'F1 Score': [0.5335413416536662,\n",
       "  0.8114387846291331,\n",
       "  0.7676056338028169,\n",
       "  0.6555183946488294,\n",
       "  0.753939393939394,\n",
       "  0.7449392712550608]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl = dataloader.get_test_dataloader()\n",
    "all_binary_results, all_true_labels, metrics_dict = synthesis(model, test_dl, best_thresholds, device)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dl = dataloader.get_test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_binary_results = []\n",
    "# all_true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for test_batch in tqdm(test_dl):\n",
    "#         raw, exam_id, label = test_batch\n",
    "#         ecg = get_inputs(raw).to(device)\n",
    "#         label = label.to(device).float()\n",
    "\n",
    "#         logits = model(ecg)\n",
    "#         probs = torch.sigmoid(logits)\n",
    "\n",
    "#         binary_result = torch.zeros_like(probs)\n",
    "#         for i in range(len(best_thresholds)):\n",
    "#             binary_result[:, i] = (\n",
    "#                 probs[:, i] >= best_thresholds[i]\n",
    "#             ).float()\n",
    "\n",
    "#         # Append binary results and true labels for this batch\n",
    "#         all_binary_results.append(binary_result)\n",
    "#         all_true_labels.append(label)\n",
    "# all_binary_results = torch.cat(all_binary_results, dim=0)\n",
    "# all_true_labels = torch.cat(all_true_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_scores = []\n",
    "# precision_scores = []\n",
    "# recall_scores = []\n",
    "# f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_idx in range(num_classes):\n",
    "#     class_binary_results = all_binary_results[:, class_idx].cpu().numpy()\n",
    "#     class_true_labels = all_true_labels[:, class_idx].cpu().numpy()\n",
    "\n",
    "#     accuracy = accuracy_score(class_true_labels, class_binary_results)\n",
    "#     precision = precision_score(\n",
    "#         class_true_labels, class_binary_results, zero_division=0\n",
    "#     )\n",
    "#     recall = recall_score(\n",
    "#         class_true_labels, class_binary_results, zero_division=0\n",
    "#     )\n",
    "#     f1 = f1_score(class_true_labels, class_binary_results, zero_division=0)\n",
    "\n",
    "#     accuracy_scores.append(accuracy)\n",
    "#     precision_scores.append(precision)\n",
    "#     recall_scores.append(recall)\n",
    "#     f1_scores.append(f1)\n",
    "\n",
    "# metrics_dict = {\n",
    "#     \"Class\": dataloader.output_col,\n",
    "#     \"Accuracy\": accuracy_scores,\n",
    "#     \"Precision\": precision_scores,\n",
    "#     \"Recall\": recall_scores,\n",
    "#     \"F1 Score\": f1_scores,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
