{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "from data.load_data import LoadData\n",
    "from models.baseline import ResnetBaseline\n",
    "\n",
    "from models.moe import ResnetMoE\n",
    "from utils import synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_label = 'gate'\n",
    "model_label = 'moe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_label == 'gate':\n",
    "    from configs.gate import LoadDataConfig, Downstream_cnn_args\n",
    "\n",
    "    loader_config = LoadDataConfig()\n",
    "    resnet_config = Downstream_cnn_args()\n",
    "\n",
    "    dataloader = LoadData(**loader_config.__dict__)\n",
    "    reference = ResnetBaseline(**resnet_config.__dict__)\n",
    "\n",
    "if model_label == 'moe':\n",
    "    from configs.baseline import LoadDataConfig\n",
    "    from configs.moe import MoE_cnn_args\n",
    "\n",
    "    loader_config = LoadDataConfig()\n",
    "    moe_config = MoE_cnn_args()\n",
    "\n",
    "    dataloader = LoadData(**loader_config.__dict__)\n",
    "    reference = ResnetMoE(**moe_config.__dict__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshould"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('output/transfer_{}.pt'.format(model_label))\n",
    "# assert model.state_dict().keys() == reference.state_dict().keys()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/272 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [04:56<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5854513584574934,\n",
       "  0.8181818181818182,\n",
       "  0.802547770700637,\n",
       "  0.6628099173553719,\n",
       "  0.7417974322396577,\n",
       "  0.7488738738738739],\n",
       " [0.33, 0.46, 0.45, 0.41000000000000003, 0.52, 0.41000000000000003])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dl = dataloader.get_val_dataloader()\n",
    "best_f1s, best_thresholds = synthesis(model, val_dl, None, device)\n",
    "best_f1s, best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# from utils import get_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 6\n",
    "# thresholds = np.arange(0, 1.01, 0.01)  # Array of thresholds from 0 to 1 with step 0.01\n",
    "# predictions = {thresh: [[] for _ in range(num_classes)] for thresh in thresholds}\n",
    "# true_labels_dict = [[] for _ in range(num_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dl = dataloader.get_val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for val_batch in tqdm(val_dl):\n",
    "#         raw, exam_id, label = val_batch\n",
    "#         ecg = get_inputs(raw).to(device)\n",
    "#         label = label.to(device).float()\n",
    "\n",
    "#         logits = model(ecg)\n",
    "#         probs = torch.sigmoid(logits)\n",
    "\n",
    "#         for class_idx in range(num_classes):\n",
    "#             for thresh in thresholds:\n",
    "#                 predicted_binary = (probs[:, class_idx] >= thresh).float()\n",
    "#                 predictions[thresh][class_idx].extend(\n",
    "#                     predicted_binary.cpu().numpy()\n",
    "#                 )\n",
    "#             true_labels_dict[class_idx].extend(\n",
    "#                 label[:, class_idx].cpu().numpy()\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_thresholds = [0.5] * num_classes\n",
    "# best_f1s = [0.0] * num_classes\n",
    "\n",
    "# for class_idx in (range(num_classes)):\n",
    "#     for thresh in thresholds:\n",
    "#         f1 = f1_score(\n",
    "#             true_labels_dict[class_idx],\n",
    "#             predictions[thresh][class_idx],\n",
    "#             zero_division=0,\n",
    "#         )\n",
    "\n",
    "#         if f1 > best_f1s[class_idx]:\n",
    "#             best_f1s[class_idx] = f1\n",
    "#             best_thresholds[class_idx] = thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_f1s, best_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [03:35<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': [0.9837346608011114,\n",
       "  0.9883653623523964,\n",
       "  0.9921856911322066,\n",
       "  0.9881338272748321,\n",
       "  0.9895230377402177,\n",
       "  0.9863973141931003],\n",
       " 'Precision': [0.5544871794871795,\n",
       "  0.7575757575757576,\n",
       "  0.7388535031847133,\n",
       "  0.5836065573770491,\n",
       "  0.739454094292804,\n",
       "  0.6766355140186916],\n",
       " 'Recall': [0.5492063492063493,\n",
       "  0.8875739644970414,\n",
       "  0.8140350877192982,\n",
       "  0.6953125,\n",
       "  0.7967914438502673,\n",
       "  0.8537735849056604],\n",
       " 'F1 Score': [0.5518341307814992,\n",
       "  0.8174386920980927,\n",
       "  0.7746243739565942,\n",
       "  0.6345811051693405,\n",
       "  0.767052767052767,\n",
       "  0.7549530761209593]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl = dataloader.get_test_dataloader()\n",
    "all_binary_results, all_true_labels, metrics_dict = synthesis(model, test_dl, best_thresholds, device)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dl = dataloader.get_test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_binary_results = []\n",
    "# all_true_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for test_batch in tqdm(test_dl):\n",
    "#         raw, exam_id, label = test_batch\n",
    "#         ecg = get_inputs(raw).to(device)\n",
    "#         label = label.to(device).float()\n",
    "\n",
    "#         logits = model(ecg)\n",
    "#         probs = torch.sigmoid(logits)\n",
    "\n",
    "#         binary_result = torch.zeros_like(probs)\n",
    "#         for i in range(len(best_thresholds)):\n",
    "#             binary_result[:, i] = (\n",
    "#                 probs[:, i] >= best_thresholds[i]\n",
    "#             ).float()\n",
    "\n",
    "#         # Append binary results and true labels for this batch\n",
    "#         all_binary_results.append(binary_result)\n",
    "#         all_true_labels.append(label)\n",
    "# all_binary_results = torch.cat(all_binary_results, dim=0)\n",
    "# all_true_labels = torch.cat(all_true_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_scores = []\n",
    "# precision_scores = []\n",
    "# recall_scores = []\n",
    "# f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for class_idx in range(num_classes):\n",
    "#     class_binary_results = all_binary_results[:, class_idx].cpu().numpy()\n",
    "#     class_true_labels = all_true_labels[:, class_idx].cpu().numpy()\n",
    "\n",
    "#     accuracy = accuracy_score(class_true_labels, class_binary_results)\n",
    "#     precision = precision_score(\n",
    "#         class_true_labels, class_binary_results, zero_division=0\n",
    "#     )\n",
    "#     recall = recall_score(\n",
    "#         class_true_labels, class_binary_results, zero_division=0\n",
    "#     )\n",
    "#     f1 = f1_score(class_true_labels, class_binary_results, zero_division=0)\n",
    "\n",
    "#     accuracy_scores.append(accuracy)\n",
    "#     precision_scores.append(precision)\n",
    "#     recall_scores.append(recall)\n",
    "#     f1_scores.append(f1)\n",
    "\n",
    "# metrics_dict = {\n",
    "#     \"Class\": dataloader.output_col,\n",
    "#     \"Accuracy\": accuracy_scores,\n",
    "#     \"Precision\": precision_scores,\n",
    "#     \"Recall\": recall_scores,\n",
    "#     \"F1 Score\": f1_scores,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
